"""
AI Research & Summarization Agent Script

This script demonstrates a modular AI pipeline that handles user queries by categorizing them into 
Research, Summarization, or BOTH, and then processing them using specialized agents. After generating 
the output, a Reflection (Critic) Agent reviews and improves the final response for clarity, accuracy, 
and completeness.

Key Features:
1. Prompt Chaining: Modular prompts for Research, Summarization, Router, and Reflection agents.
2. Routing: Classifies the user's query and routes it to the appropriate agent(s).
3. Parallelization: Runs Research and Summarization agents concurrently if needed.
4. Reflection: Critic agent evaluates the output and provides an improved version.

Requirements:
- Python 3.10+
- langchain-core
- langchain-google-genai
- A valid Gemini API key set in the environment variable 'Gemini_APIKEY'.

Example Usage:
$ python ai_research_summarization_agent.py
"""

import os
from langchain_google_genai import GoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnableParallel


def build_llm_model() -> GoogleGenerativeAI:
    """
    Initialize and return the Google Generative AI model.

    Returns:
        GoogleGenerativeAI: Configured LLM model with Gemini API key.
    """
    return GoogleGenerativeAI(
        model="gemini-2.5-flash",
        api_key=os.getenv("Gemini_APIKEY"),
    )

def build_research_template() -> ChatPromptTemplate:
    """
    Build a highly skilled Research Agent prompt template.

    Returns:
        ChatPromptTemplate: Prompt template for the Research Agent.
    """
    return ChatPromptTemplate([
        ("system",
         """
         You are a highly skilled Research Agent.
         Your role is to search, analyze, and extract key insights from multiple sources 
         about the userâ€™s query. Provide accurate, detailed, structured information, 
         key terms, definitions, examples, and avoid generic summaries.
         """),
        ("user", "Topic: {query}")
    ])


def build_summarization_template() -> ChatPromptTemplate:
    """
    Build a specialized Summarization Agent prompt template.

    Returns:
        ChatPromptTemplate: Prompt template for the Summarization Agent.
    """
    return ChatPromptTemplate([
        ("system",
         """
         You are a Summarization Agent.
         Condense raw research text or multiple information chunks into a clear, concise, 
         and structured summary. Retain essential meaning, key arguments, and critical data.
         """),
        ("user", "Topic: {query}")
    ])


def build_router_template() -> ChatPromptTemplate:
    """
    Build a Router Agent prompt template for query classification.

    Returns:
        ChatPromptTemplate: Prompt template for routing the query.
    """
    return ChatPromptTemplate([
        ("system",
         """
         You are a Router Agent.
         Classify the user's query into one of three categories: 'Research', 'Summarization', or 'BOTH'.
         Return only the category.
         """),
        ("user", "Topic: {query}")
    ])


def build_reflection_template() -> ChatPromptTemplate:
    """
    Build a Reflection (Critic) Agent prompt template.

    Returns:
        ChatPromptTemplate: Prompt template for reflection and critique.
    """
    return ChatPromptTemplate([
        ("system",
         """
         You are a Reflection Agent (Critic).
         Review the output generated by the Research and/or Summarization Agents.
         Identify weaknesses such as lack of clarity, missing details, over-generalization, redundancy, or factual gaps.
         Provide an improved version that is accurate, clear, and well-structured.
         Structure your output in two parts:
         1. Critique: List problems or improvements needed.
         2. Improved Answer: Refined, polished version suitable as the final response.
         """),
        ("user", "Here is the agent's response to reflect on:\n\n{agent_output}")
    ])


def create_routing_branches(research_chain, summarization_chain, both_chain) -> RunnableBranch:
    """
    Create a branching router to route queries to the appropriate agent(s).

    Args:
        research_chain: Research agent runnable chain.
        summarization_chain: Summarization agent runnable chain.
        both_chain: Runnable chain that executes both agents in parallel.

    Returns:
        RunnableBranch: Configured branch runnable for routing.
    """
    return RunnableBranch(
        (lambda x: x['decision'].strip() == "Research", research_chain),
        (lambda x: x['decision'].strip() == "Summarization", summarization_chain),
        (lambda x: x['decision'].strip() == "BOTH", both_chain),
        research_chain  # default branch
    )


def get_both_chain(research_chain, summarization_chain) -> RunnableParallel:
    """
    Combine Research and Summarization chains to run in parallel.

    Args:
        research_chain: Research agent runnable chain.
        summarization_chain: Summarization agent runnable chain.

    Returns:
        RunnableParallel: Parallel execution of both agents.
    """
    return RunnableParallel({
        "research_results": research_chain,
        "summarization_results": summarization_chain
    })


def main():
    """Main execution function for the AI Research & Summarization Agent."""
    user_query = "Explain and summarize the applications of blockchain in supply chains"

    # Build agent templates
    research_agent_template = build_research_template()
    summarization_agent_template = build_summarization_template()
    router_agent_template = build_router_template()
    critic_agent_template = build_reflection_template()

    # Initialize LLM
    llm = build_llm_model()

    # Create agent chains
    research_chain = (research_agent_template | llm | StrOutputParser())
    summarization_chain = (summarization_agent_template | llm | StrOutputParser())
    router_chain = (router_agent_template | llm | StrOutputParser())
    critic_chain = (critic_agent_template | llm | StrOutputParser())

    # Parallel chain for both Research and Summarization
    both_chain = get_both_chain(research_chain, summarization_chain)
    router_branch = create_routing_branches(research_chain, summarization_chain, both_chain)

    # Step 1: Route user query
    router_agent_response = router_chain.invoke({'query': user_query})
    print(f"User Query: {user_query}")
    print(f"Router Agent's Decision: {router_agent_response}")
    print(f"\nRouting to {router_agent_response.lower()}_branch")

    # Step 2: Execute routed chain
    router_branch_input = {'decision': router_agent_response, 'query': user_query}
    agent_response = router_branch.invoke(router_branch_input)
    print(f"\nOriginal response: {agent_response}")

    # Step 3: Reflection / Critique
    critic_agent_response = critic_chain.invoke({"agent_output": agent_response})
    print(f"\nCritic Agent's Reflected Response: {critic_agent_response}")


if __name__ == "__main__":
    main()