"""
Restaurant Name Generator using LangChain + Google Gemini AI
------------------------------------------------------------

This script demonstrates how to use LangChain chains and pipelines 
to generate creative restaurant names based on cuisine type.

Concepts Covered:
- PromptTemplate: For formatting user input into model-friendly prompts.
- LLM: ChatGoogleGenerativeAI model to generate text.
- StrOutputParser: Extracts clean text from LLM responses.
- Chain / Pipeline: Connects multiple steps (prompt -> LLM -> parser) into a single workflow.

Requirements:
- Python 3.9+
- Packages:
    pip install langchain-google-genai langchain-core

Environment Variables:
- GOOGLE_API_KEY: API key for Google Gemini AI.
"""

import os
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_core.messages import HumanMessage
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ---------------------------------------------------------------------
# Step 1: Initialize the LLM (Language Model)
# ---------------------------------------------------------------------
# ChatGoogleGenerativeAI is the interface to Google's Gemini model.
# - model: specific Gemini version to use
# - google_api_key: authenticate requests
# - max_output_tokens: limit response length
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    max_output_tokens=200
)

# ---------------------------------------------------------------------
# Step 2: Initialize the Output Parser
# ---------------------------------------------------------------------
# StrOutputParser ensures we get clean text from the model output
# (removes metadata, formatting, or extra tokens)
output_parser = StrOutputParser()

# ---------------------------------------------------------------------
# Step 3: Define the Prompt Template
# ---------------------------------------------------------------------
# PromptTemplate formats the input into instructions the model can understand.
# - input_variables: placeholders to replace in the template
# - template: the actual prompt text
prompt_template = PromptTemplate(
    input_variables=["cuisine"],
    template=(
        "I want to open a restaurant for {cuisine} Food. "
        "Suggest me a fancy name. Do NOT give any explanation or meaning."
    )
)

# ---------------------------------------------------------------------
# Step 4: Create the Chain / Pipeline
# ---------------------------------------------------------------------
# Combine the prompt, LLM, and output parser into a single workflow.
# This is the "pipeline" where the output of one step flows into the next.
# Chain flow: PromptTemplate -> LLM -> StrOutputParser
name_generation_chain = prompt_template | llm | output_parser

# ---------------------------------------------------------------------
# Step 5: Generate Restaurant Name
# ---------------------------------------------------------------------
# Invoke the chain with specific input for the placeholder {cuisine}.
# - Input: dictionary with keys matching input_variables
# - Output: clean text generated by the LLM
restaurant_name = name_generation_chain.invoke({"cuisine": "Mexican"})

# ---------------------------------------------------------------------
# Step 6: Display the Result
# ---------------------------------------------------------------------
# Prints the generated restaurant name
print(f"Suggested Restaurant Name for Mexican Cuisine: {restaurant_name}")